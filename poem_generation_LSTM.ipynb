{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"hw5_99131009.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1QzxV7IQRce9wY3YiWeP_ceTXrILjA-_Y","authorship_tag":"ABX9TyMeBj3mehf82foG/xeJg41h"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"V8UJRMo8y1B4"},"source":["#  word-based model"]},{"cell_type":"code","metadata":{"id":"hKI4nWFJH4ST"},"source":["import tensorflow as tf\n","from tensorflow.keras.layers.experimental import preprocessing\n","import numpy as np\n","import os\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","import tensorflow.keras.utils as ku\n","from nltk.translate.bleu_score import sentence_bleu\n","from matplotlib.pyplot import plot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-4EmIsShIG2q"},"source":["# read the data"]},{"cell_type":"code","metadata":{"id":"pNjAg9_zH5EV"},"source":["os.chdir('/content/drive/My Drive/ANN/hw5/shahname')\n","path_to_file = 'shahname'\n","listOfFiles = os.listdir()\n","poems = []\n","for f in listOfFiles:\n","  poems.append(open(f).read())\n","os.chdir('/content/drive/My Drive/ANN/hw5')\n","dictionary = open('allShahnameWords.txt').readlines()\n","dictionary.append(\"گهٔ\\n\")\n","dictionary.append(\"رهٔ\\n\")\n","dictionary.append(\"فرهٔ\\n\")\n","dictionary.append(\"**\\n\")  #مصرع\n","dictionary.append(\"&&&\\n\")    #بیت"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDXNazH7SFfV"},"source":["test = poems[561:]\n","poems_text = \"\"\n","for i in range(561):\n","  poems_text += poems[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3RMiMkwyxner"},"source":["# sentences = []\n","# for poem in poems:\n","#   for verse in poem:\n","#     s = []\n","#     words = str.split(verse)\n","#     for word in words: \n","#       s.append(dictionary.index(word+'\\n'))\n","#     sentences.append(s)\n","# padded_sentences = tf.keras.preprocessing.sequence.pad_sequences(sentences)\n","\n","# vocab = sorted(set(poems))\n","# sentences = np.zeros([len(poems)//100, 100])\n","# for i in range(len(poems)//100):\n","#   for j in range(100):\n","#     sentences[i,j] = vocab.index(poems[100*i + j])\n","\n","poems1 = poems_text.replace(\"\\t\",\" ** \").replace(\"\\n\",\" &&& \")\n","words = str.split(poems1)\n","sentences = np.zeros([len(words)//15,15])\n","for i in range(len(words)//15):\n","  for j in range(15):\n","    sentences[i,j] = dictionary.index(words[15*i + j]+'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fEiZ7Rnp9gpO"},"source":["# create dataset"]},{"cell_type":"code","metadata":{"id":"uk4oAhWA2TYI"},"source":["X_train = sentences[:,:-1]\n","y_train = sentences[:,1:]\n","for i in range(1,len(test)):\n","  test[i] = str.split(test[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLjFKpi13bxC"},"source":["# build the model"]},{"cell_type":"code","metadata":{"id":"fz4h1vj8dMRX"},"source":["class EarlyStoppingCallback(keras.callbacks.Callback):\n","  def __init__(self,patience=0):\n","    super(EarlyStoppingCallback,self).__init__()\n","    self.patience = patience\n","    self.val_acc = []\n","    self.trn_acc = []\n","    self.cur_epoch = 0\n","\n","  def on_train_begin(self,logs=None):\n","    self.best = -1 * np.Inf\n","    self.wait  = 0\n","    self.stopped_epoch = 0\n","  def on_epoch_end(self,epoch,logs =None):\n","    current_acc = logs.get(\"val_accuracy\")\n","    self.val_acc.append(current_acc)\n","    self.trn_acc.append(logs.get(\"accuracy\"))\n","    self.cur_epoch = epoch\n","    if np.greater(current_acc,self.best):\n","      self.best = current_acc\n","      self.wait = 0\n","      print(\"******************\")\n","      print(current_acc)\n","      self.best_weights = self.model.get_weights()\n","    else: \n","      self.wait +=1 \n","      if self.wait > self.patience :\n","        print(\"in stop scope\")\n","        self.stopped_epoch = epoch - self.patience\n","        self.model.stop_training = True\n","        self.model.set_weights(self.best_weights)\n","  def on_train_end(self,logs = None):\n","    if self.stopped_epoch > 0 :\n","      print(\"epoch: %d: early stopping\" , self.cur_epoch)\n","      print(self.best)\n","    plt.plot(range(self.cur_epoch+1),self.val_acc,label=\"val_acc\",color = 'orange')\n","    plt.plot(range(self.cur_epoch+1),self.trn_acc,label=\"train_acc\", color='blue')\n","    plt.legend()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"uNAYyMJu3eiY"},"source":["def loss(labels,logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)\n","er_cal = EarlyStoppingCallback(patience=10)\n","vocab_size = len(dictionary)\n","# vocab_size = len(vocab)\n","embedding_dim = 250\n","rnn_units = 700\n","learning_rate = 0.001\n","model = keras.models.Sequential()\n","model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim))\n","model.add(tf.keras.layers.LSTM(rnn_units,return_sequences = True))\n","model.add(tf.keras.layers.Dense(vocab_size))\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),loss=loss,metrics=['accuracy']) \n","model.fit(X_train,y_train,validation_split=0.22,batch_size=256, epochs=2, callbacks=[er_cal])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDeSRnorTt74"},"source":["# generating phase"]},{"cell_type":"code","metadata":{"id":"nqbQH8CCsqSI"},"source":["score = 0\n","for j,t in enumerate(test): \n","  generated_poem = t[0]\n","  words_id = [dictionary.index(t[0]+'\\n')]\n","  for i in range(len(t)):\n","    words_id.append(model.predict([words_id])[0,-1].argmax())\n","    generated_poem  +=  \" \" + dictionary[int(words_id[-1])].strip('\\n')\n","  score += sentence_bleu([t], str.split(generated_poem))\n","  generated_poem1 = generated_poem.replace(\"**\",\"\\t\").replace(\"&&&\",\"\\n\")\n","  print(generated_poem1)\n","  print(\"--------------------\")\n","print(\"BLEU score : \" , score / len(X_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mfuQgPg5yH0d"},"source":["# character-based  model"]},{"cell_type":"code","metadata":{"id":"_c4yrUGXs04q"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM,Dense,Embedding\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","import os\n","from nltk.translate.bleu_score import sentence_bleu\n","from tensorflow.keras.models import load_model\n","from tensorflow import keras\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GK0cVYSzyVdl"},"source":["# create dataset"]},{"cell_type":"code","metadata":{"id":"y4M6Zrhpt9HL"},"source":["os.chdir('/content/drive/My Drive/ANN/shahname')\n","path_to_file = 'shahname'\n","listOfFiles = os.listdir()\n","poems = []\n","for f in listOfFiles:\n","  poems.append(open(f,'r').read())\n","\n","test = poems[561:]\n","poems_text = \"\"\n","for i in range(561):\n","  poems_text += poems[i]\n","\n","vocab = sorted(set(poems_text))\n","\n","\n","char_to_ind = {char:i for i, char in enumerate(vocab)}\n","ind_to_char = np.array(vocab)\n","encoded_text = np.array([char_to_ind[c] for c in poems_text])\n","\n","\n","seq_len = 200\n","\n","char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n","sequences = char_dataset.batch(seq_len+1, drop_remainder=True)\n","\n","def create_seq_targets(seq):\n","    input_txt = seq[:-1]\n","    target_txt = seq[1:]\n","    return input_txt, target_txt\n","    \n","dataset = sequences.map(create_seq_targets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t0SyBAX0yiNx"},"source":["# build model"]},{"cell_type":"code","metadata":{"id":"Itk3CRRhuGX6"},"source":["# Batch size\n","batch_size = 128\n","\n","dataset = dataset.batch(batch_size, drop_remainder=True)\n","\n","def sparse_cat_loss(y_true,y_pred):\n","  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n","\n","def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n","    model = Sequential()\n","    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n","    model.add(LSTM(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n","    # Final Dense Layer to Predict\n","    model.add(Dense(vocab_size))\n","    model.compile(optimizer='adam', loss=sparse_cat_loss,metrics=['accuracy']) \n","    return model\n","  \n","  \n","  \n","# Length of the vocabulary in chars\n","vocab_size = len(vocab)\n","# The embedding dimension\n","embed_dim = 50\n","# Number of RNN units\n","rnn_neurons = 100\n","\n","#Create the model\n","model = create_model(\n","  vocab_size = vocab_size,\n","  embed_dim=embed_dim,\n","  rnn_neurons=rnn_neurons,\n","  batch_size=batch_size)\n","\n","\n","#Train the model\n","epochs = 30\n","model.fit(dataset,epochs=epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-3w54OmLynVN"},"source":["# save model"]},{"cell_type":"code","metadata":{"id":"KUCpxZ23yE-t"},"source":["os.chdir('/content/drive/My Drive/ANN')\n","model.save('shahname_gen2.h5') \n","\n","\n","model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n","model.load_weights('shahname_gen2.h5')\n","model.build(tf.TensorShape([1, None]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ggtnf9bZypck"},"source":["# generate text"]},{"cell_type":"code","metadata":{"id":"XBdI-fHMud7W"},"source":["def generate_text(model, start_seed,gen_size=100,temp=0.5):\n","\n","  num_generate = gen_size\n","  input_eval = [char_to_ind[s] for s in start_seed]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","  text_generated = []\n","  predicts = []\n","  temperature = temp\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval)\n","      predictions = tf.squeeze(predictions, 0)\n","      predictions = predictions / temperature\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","      input_eval = tf.expand_dims([predicted_id], 0)\n","      text_generated.append(ind_to_char[predicted_id])\n","      predicts.append(predicted_id)\n","  return (start_seed + ''.join(text_generated)), predicts\n","\n","test_t = []\n","for t in test:\n","  test_t.append(t[0:200])\n","\n","score = 0\n","val_acc = 0\n","for i in range(len(test_t)):\n","  generated_poem , predicted_labels = generate_text(model,test_t[i][0:15],gen_size=len(test_t[i])-15)\n","  true_labels = [char_to_ind[s] for s in test_t[i][15:]]\n","  val_acc += accuracy_score(true_labels,predicted_labels)\n","  # print(val_acc)\n","  score += sentence_bleu([str.split(test_t[i])], str.split(generated_poem))\n","  print(generated_poem)\n","  print(\"-------------------------\")\n","print(\"BLEU score : \" , score / len(test_t))\n","print(\"validation accuracy : \", val_acc/len(test_t))"],"execution_count":null,"outputs":[]}]}